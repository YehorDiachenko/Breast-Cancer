{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попередня обробка даних\n",
    "\n",
    "### Вступ\n",
    "\n",
    "Попередня обробка даних - це важливий етап в аналізі даних. Необхідно заздалегідь підготувати дані таким чином, щоб найкраще розкрити структуру проблеми алгоритмам машинного навчання, які будуть до цих даних застосовуватись. Це передбачає ряд заходів, таких як:\n",
    "* Присвоєння числових значень категоричним даним;\n",
    "* Обробка відсутніх значень;\n",
    "* Нормалізація функцій (щоб функції на малих масштабах не домінували при підключенні моделі до даних).\n",
    "\n",
    "\n",
    "\n",
    "У розділі 2 \"Дослідницький аналіз даних\" виконане дослідження, щоб допомогти зрозуміти розподіл даних, а також те, як атрибути співвідносяться між собою. Були визначені функції інтересу. У цьому розділі використовуються функції для оптимізації даних великих розмірів.\n",
    "\n",
    "### Мета:\n",
    "Знайти найбільш прогнозовані функції даних і відфільтрувати їх так, щоб це підвищило прогнозовану силу аналітичної моделі.\n",
    "\n",
    "#### Завантаження даних та основних бібліотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Завантаження бібліотек для обробки даних\n",
    "import pandas as pd #обробка даних, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# візуалізація\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'\n",
    "\n",
    "data = pd.read_csv('data/clean-data.csv', index_col=False)\n",
    "data.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодування міток\n",
    "Занесення 30 ознак до NumPy масиву X та перетворення класових міток з їх оригінального рядкового (string) представлення (M та B) в цілочисельне (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Призначення предикторів до змінної типу ndarray (матриця)\n",
    "array = data.values\n",
    "X = array[:,1:31]\n",
    "y = array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#перетворення класових міток з їх оригінального рядкового (string) представлення (M та B) в цілочисельне (integer)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#метод перетворення LabelEncorder двох фіктивних змінних\n",
    "#le.transform (['M', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після кодування міток класу (діагнозу) у масиві злоякісні пухлини тепер представлені як клас 1 (тобто наявність ракових клітин), а доброякісні пухлини представлені як клас 0 (тобто відсутність ракових клітин) відповідно.\n",
    "\n",
    "\n",
    "#### Оцінка точності моделі: розбиття даних на навчальні та тестові набори\n",
    "\n",
    "Найпростіший метод оцінювання продуктивності алгоритму машинного навчання - використання різних наборів даних для навчання та тестування. Кроки:\n",
    "* Розділити наявні дані на навчальний і тестовий набір. (70% навчання, 30% тесту)\n",
    "* Навчання алгоритму на першій частині даних\n",
    "* Зробити прогноз щодо другої частини\n",
    "* Оцінити прогнози щодо очікуваних результатів\n",
    "\n",
    "Розмір розбиття може залежати від розміру та специфіки набору даних, хоча зазвичай 67% даних використовується для тренувань, а решта 33% - для тестування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Розбиття набору даних: 70% для навчання, 30% для тестування\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=7)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стандартизація ознак\n",
    "\n",
    "* Стандартизація - це корисна методика перетворення атрибутів з гауссовим розподілом та різними відхиленнями у стандартний розподіл Гаусса із середнім значенням 0 та стандартним відхиленням 1.\n",
    "\n",
    "* Як вже відомо з розділу 2 \"Дослідницький аналіз даних\" вихідні дані мають різні розподіли, що може впливати на більшість алгоритмів машинного навчання. Алгоритми машинного навчання та оптимізації поводяться набагато краще, якщо функції знаходяться в одному масштабі.\n",
    "\n",
    "Оцінимо ті самі алгоритми зі стандартизованою копією набору даних. Використовується sklearn для масштабування та перетворення даних таким чином, що кожен атрибут має середнє значення 0 та стандартне відхилення 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# нормалізація даних (центр близький до 0 і масштаб для видалення дисперсії).\n",
    "scaler =StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Декомпозиція ознак використовуючи аналіз основних компонентів - Principal Component Analysis (PCA)\n",
    "В розділі 2 \"Дослідницький аналіз даних\" були наведені пари ознак, які добре розділяють дані. Тому є сенс використовувати один із методів зменшення розмірності, щоб спробувати використовувати якомога більше функцій та підтримувати якомога більше інформації при роботі з лише двома вимірами. Далі використовується PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# вилучення ознак\n",
    "pca = PCA(n_components=10)\n",
    "fit = pca.fit(Xs)\n",
    "\n",
    "# підсумкові дані\n",
    "#print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "#print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(Xs)\n",
    "\n",
    "PCA_df = pd.DataFrame()\n",
    "\n",
    "PCA_df['PCA_1'] = X_pca[:,0]\n",
    "PCA_df['PCA_2'] = X_pca[:,1]\n",
    "\n",
    "plt.plot(PCA_df['PCA_1'][data.diagnosis == 'M'],PCA_df['PCA_2'][data.diagnosis == 'M'],'o', alpha = 0.7, color = 'r')\n",
    "plt.plot(PCA_df['PCA_1'][data.diagnosis == 'B'],PCA_df['PCA_2'][data.diagnosis == 'B'],'o', alpha = 0.7, color = 'b')\n",
    "\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "plt.legend(['Malignant','Benign'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після застосування лінійної трансформації PCA отримано підпростір (від 3D до 2D), де зразки \"найбільш розповсюджені\" уздовж нових осей функції."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#кількість дисперсій, яку пояснює кожен головний компонент\n",
    "var= pca.explained_variance_ratio_\n",
    "#сукупна варіативність\n",
    "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вирішення, скільки основних компонентів потрібно зберегти\n",
    "Для того, щоб визначити, скільки основних компонентів слід зберегти, прийнято узагальнити результати аналізу основних компонентів, склавши графік огляду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#кількість дисперсій, яку пояснює кожен головний компонент\n",
    "var= pca.explained_variance_ratio_\n",
    "#сукупна варіативність\n",
    "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#print(var1)\n",
    "\n",
    "plt.plot(var)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3,shadow=False,markerscale=0.4)\n",
    "leg.get_frame().set_alpha(0.4)\n",
    "leg.draggable(state=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найбільш очевидна зміна нахилу відбувається в компоненті 2, який є «ліктем» графіку власних значень. Таким чином, виходячи з графіку, можна стверджувати, що перші три компоненти слід зберегти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Підсумок:\n",
    "\n",
    "1. Занесення 30 ознак до NumPy масиву X та перетворення класових міток з їх оригінального рядкового представлення (M та B) в цілочисельне.\n",
    "2. Розбиття даних на навчальні та тестові набори.\n",
    "3. Стандартизація даних.\n",
    "4. Отримання власних векторів та значень з матриці коваріації чи матриці кореляції.\n",
    "5. Сортування власних значень у порядку зменшення та вибір власних векторів kk, які відповідають найбільшим власним значенням kk, де k - розмірність нового підпростору ознак (k≤dk≤d).\n",
    "6. Побудова проекційної матриці W з вибраних власних векторів k.\n",
    "7. Перетворення вихідного набору даних X через W, щоб отримати k-мірний підпростір ознак Y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
